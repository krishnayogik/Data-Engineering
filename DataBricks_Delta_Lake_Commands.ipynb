{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Table in DataBricks - Example"
      ],
      "metadata": {
        "id": "DtIGfoF79PZ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RD9hFNEpIDQ"
      },
      "outputs": [],
      "source": [
        " CREATE TABLE employee_delta (\n",
        "      empno INT,\n",
        "      ename STRING,\n",
        "      designation STRING,\n",
        "      manager INT,\n",
        "      hire_date DATE,\n",
        "      sal BIGINT,\n",
        "      deptno INT,\n",
        "      location STRING\n",
        ") USING DELTA;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Table from Path - Example\n",
        "\n"
      ],
      "metadata": {
        "id": "I_6_Wlli9kH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " CREATE TABLE employee_delta (\n",
        "      empno INT,\n",
        "      ename STRING,\n",
        "      designation STRING,\n",
        "      manager INT,\n",
        "      hire_date DATE,\n",
        "      sal BIGINT,\n",
        "      deptno INT,\n",
        "      location STRING\n",
        ") USING DELTA\n",
        "Location '/mnt/bdpdatalake/blob-storage/';"
      ],
      "metadata": {
        "id": "xwsqAxBP9gCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Table with Partition - Example"
      ],
      "metadata": {
        "id": "2cLdMmw49r4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE employee_delta (\n",
        "      empno INT,\n",
        "      ename STRING,\n",
        "      manager INT,\n",
        "      hire_date DATE,\n",
        "      sal BIGINT,\n",
        "      deptno INT,\n",
        "      location STRING\n",
        ") PARTITION BY (\n",
        "      designation STRING\n",
        ")\n",
        "USING DELTA\n",
        "Location '/mnt/bdpdatalake/blob-storage/';"
      ],
      "metadata": {
        "id": "k4LkmvbY975Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta Table from Dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "NgDZsXv09_0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " df.write.format(\"delta\").saveAsTable(\"testdb.testdeltatable\")"
      ],
      "metadata": {
        "id": "4kLabDri-Ucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify\n",
        "%sql\n",
        "show create table testdb.testdeltatable;"
      ],
      "metadata": {
        "id": "1PYVC6mG-Wez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta Table From Dataframe Without Schema At External Location"
      ],
      "metadata": {
        "id": "qP9eH_Pg-mKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %sql\n",
        "CREATE TABLE testdb.testDeltaTable2\n",
        "USING DELTA\n",
        "LOCATION '/mnt/blob-storage/testDeltaTable2'\n",
        "\n",
        "%scala\n",
        "df.write \\\n",
        "  .format(\"delta\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .save(\"mnt/blob-storage/\")"
      ],
      "metadata": {
        "id": "touEHAUI-ZsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify\n",
        "%sql\n",
        "show create table testdb.testdeltatable2;"
      ],
      "metadata": {
        "id": "VtlZfzfu-uyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Parquet Table from CSV File in Databricks"
      ],
      "metadata": {
        "id": "VBL7C51A-rvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "# File location and type\n",
        "file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"false\"\n",
        "first_row_is_header = \"false\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)\n",
        "\n",
        "\n",
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"emp_data13_csv\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)\n",
        "\n",
        "\n",
        "%sql\n",
        "\n",
        "/* Query the created temp table in a SQL cell */\n",
        "\n",
        "select * from `emp_data13_csv`\n",
        "\n",
        "\n",
        "permanent_table_name = \"emp_data13_csv\"\n",
        "\n",
        "# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"
      ],
      "metadata": {
        "id": "FR6YnSDID0Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta Table from CSV File in Databricks"
      ],
      "metadata": {
        "id": "9DCDvz2MD-Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "# File location and type\n",
        "file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"false\"\n",
        "first_row_is_header = \"false\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)\n",
        "\n",
        "\n",
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"emp_data13_csv\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)\n",
        "\n",
        "\n",
        "%sql\n",
        "\n",
        "/* Query the created temp table in a SQL cell */\n",
        "\n",
        "select * from `emp_data13_csv`\n",
        "\n",
        "\n",
        "permanent_table_name = \"emp_data13_csv\"\n",
        "\n",
        "# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"
      ],
      "metadata": {
        "id": "ovEvf0jrEdMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta Table with Partition from CSV File in Databricks\n"
      ],
      "metadata": {
        "id": "pU3SzbYjEhXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load csv\n",
        "%scala\n",
        "val file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "\n",
        "val df = spark.read.format(\"csv\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"sep\", \",\")\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "0Fr5S3yOEn8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df.write.format(\"delta\").partitionBy(\"location\").saveAsTable(testdb.emp_partition_tbl)"
      ],
      "metadata": {
        "id": "djQUno_IEvcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show partitions testdb.emp_partition_tbl"
      ],
      "metadata": {
        "id": "xEI1svX9Eygg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta Table from JSON File in Databricks"
      ],
      "metadata": {
        "id": "JpYUS7vAE087"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "val jsonDf = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/tables/emp_data1.json\")\n",
        "display(jsonDf)"
      ],
      "metadata": {
        "id": "Ux3WKarSE6Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " jsonDf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"testdb.jsonDataTable\")"
      ],
      "metadata": {
        "id": "Sgs-LzevE9dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show create table testdb.jsondatatable;"
      ],
      "metadata": {
        "id": "ZTBmnnQ9E_hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta table from Excel File in Databricks"
      ],
      "metadata": {
        "id": "ViYMfMp5FB95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val xslFilePath = \"/FileStore/tables/emp_data1.xls\"\n",
        "val xslDf = spark.read.format(\"com.crealytics.spark.excel\")\n",
        "                      .option(\"header\", \"true\")\n",
        "                      .option(\"dataAddress\", \"sheet1\")\n",
        "                      .load(xslFilePath)\n",
        "display(xslDf)"
      ],
      "metadata": {
        "id": "TNdn-iJnFI9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " xslDf.write.format(\"delta\").saveAsTable(\"excel_tableName\")"
      ],
      "metadata": {
        "id": "GXWsTFT2FMy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta table from TSV File in Databricks\n"
      ],
      "metadata": {
        "id": "y6z-NruDFV50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "val tsvFilePath = \"/FileStore/tables/emp_data1.tsv\"\n",
        "\n",
        "val tsvDf = spark.read.format(\"csv\")\n",
        "                      .option(\"header\", \"true\")\n",
        "                      .option(\"sep\", \"\\t\")\n",
        "                      .load(tsvFilePath)\n",
        "\n",
        "display(tsvDf)"
      ],
      "metadata": {
        "id": "gHEm_MvTFOgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Nested JSON in Spark DataFrame\n"
      ],
      "metadata": {
        "id": "4MiWLOsXFaKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "// Step 1: Load Nested JSON data into Spark Dataframe\n",
        "val ordersDf = spark.read.format(\"json\")\n",
        "                         .option(\"inferSchema\", \"true\")\n",
        "                         .option(\"multiLine\", \"true\")\n",
        "                         .load(\"/FileStore/tables/orders_sample_datasets.json\")\n",
        "\n",
        "// Step 2: Explode -\n",
        "var parseOrdersDf = ordersDf.withColumn(\"orders\", explode($\"datasets\"))\n",
        "\n",
        "// Step 3: Fetch Each Order using getItem on explode column\n",
        "parseOrdersDf = parseOrdersDf.withColumn(\"customerId\", $\"orders\".getItem(\"customerId\"))\n",
        "                             .withColumn(\"orderId\", $\"orders\".getItem(\"orderId\"))\n",
        "                             .withColumn(\"orderDate\", $\"orders\".getItem(\"orderDate\"))\n",
        "                             .withColumn(\"orderDetails\", $\"orders\".getItem(\"orderDetails\"))\n",
        "                             .withColumn(\"shipmentDetails\", $\"orders\".getItem(\"shipmentDetails\"))\n",
        "\n",
        "// Step 4: Explode orderDetails column to flatten all the rows\n",
        "parseOrdersDf = parseOrdersDf.withColumn(\"orderDetails\", explode($\"orderDetails\"))\n",
        "\n",
        "// Step 5: Fetch attributes from object and make it available in a column\n",
        "parseOrdersDf = parseOrdersDf.withColumn(\"productId\", $\"orderDetails\".getItem(\"productId\"))\n",
        "                             .withColumn(\"quantity\", $\"orderDetails\".getItem(\"quantity\"))\n",
        "                             .withColumn(\"sequence\", $\"orderDetails\".getItem(\"sequence\"))\n",
        "                             .withColumn(\"totalPrice\", $\"orderDetails\".getItem(\"totalPrice\"))\n",
        "                             .withColumn(\"city\", $\"shipmentDetails\".getItem(\"city\"))\n",
        "                             .withColumn(\"country\", $\"shipmentDetails\".getItem(\"country\"))\n",
        "                             .withColumn(\"postalcode\", $\"shipmentDetails\".getItem(\"postalCode\"))\n",
        "                             .withColumn(\"street\", $\"shipmentDetails\".getItem(\"street\"))\n",
        "                             .withColumn(\"state\", $\"shipmentDetails\".getItem(\"state\"))\n",
        "\n",
        "// Step 6: Fetch gross, net and tax from totalprice object\n",
        "parseOrdersDf = parseOrdersDf.withColumn(\"gross\", $\"totalprice\".getItem(\"gross\"))\n",
        "                             .withColumn(\"net\", $\"totalprice\".getItem(\"net\"))\n",
        "                             .withColumn(\"tax\", $\"totalprice\".getItem(\"tax\"))\n",
        "\n",
        "// Step 7: Select required columns from the dataframe\n",
        "val jsonParseOrdersDf = parseOrdersDf.select($\"orderId\"\n",
        "                                           ,$\"customerId\"\n",
        "                                           ,$\"orderDate\"\n",
        "                                           ,$\"productId\"\n",
        "                                           ,$\"quantity\"\n",
        "                                           ,$\"sequence\"\n",
        "                                           ,$\"gross\"\n",
        "                                           ,$\"net\"\n",
        "                                           ,$\"tax\"\n",
        "                                           ,$\"street\"\n",
        "                                           ,$\"city\"\n",
        "                                           ,$\"state\"\n",
        "                                           ,$\"postalcode\"\n",
        "                                           ,$\"country\")\n",
        "\n",
        "display(jsonParseOrdersDf)"
      ],
      "metadata": {
        "id": "p0lkifRfGgkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write DataFrame to Delta Table in Databricks with Append Mode"
      ],
      "metadata": {
        "id": "hTrCnbQeGlci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "\n",
        "val df = spark.read.format(\"csv\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"sep\", \",\")\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "jYZ3kV-fGwJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write DataFrame to Delta Table in Databricks with Overwrite Mode"
      ],
      "metadata": {
        "id": "ppG3B16qIfbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "\n",
        "val df = spark.read.format(\"csv\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"sep\", \",\")\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "nSaoOiNgIzv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(permanent_table_name)"
      ],
      "metadata": {
        "id": "ekg-VR2hI3YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Delta Data in Databricks"
      ],
      "metadata": {
        "id": "L4Qqzq1eI4LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "select * from delta.`/mnt/blob-storage/testDeltaTable2/`"
      ],
      "metadata": {
        "id": "urHVzWbaI8Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge into Delta Table using Spark SQL"
      ],
      "metadata": {
        "id": "l2zBn4jKJh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "val dailyDf = Seq((1400, \"Person4\", \"Location4\", \"Contact4\")\n",
        "                 ,(1500, \"Person5\", \"Location5\", \"Contact5\")\n",
        "                 ,(1600, \"Person6\", \"Location6\", \"Contact6\")).toDF(\"id\", \"name\", \"location\", \"contact\")\n",
        "dailyDf.createOrReplaceTempView(\"dailyTable\")"
      ],
      "metadata": {
        "id": "GSCW_crvJrpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MERGE INTO testdb.testdeltatable as target\n",
        "USINg dailyTable as source\n",
        "ON target.id = source.id\n",
        "WHEN MATCHED\n",
        "  THEN UPDATE SET *\n",
        "WHEN NOT MATCHED\n",
        "  THEN INSERT *"
      ],
      "metadata": {
        "id": "0ORb_H83JnI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge into Delta Table using Spark Scala"
      ],
      "metadata": {
        "id": "uo7oVrevJv41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "import io.delta.tables._\n",
        "\n",
        "val dailyDf = Seq((1400, \"Person4\", \"Location4\", \"Contact4\")\n",
        "                 ,(1500, \"Person5\", \"Location5\", \"Contact5\")\n",
        "                 ,(1600, \"Person6\", \"Location6\", \"Contact6\")).toDF(\"id\", \"name\", \"location\", \"contact\")\n",
        "\n",
        "val target_table = DeltaTable.forName(\"testdb.testDeltaTable\")\n",
        "\n",
        "target_table.as(\"target\")\n",
        "  .merge(\n",
        "    dailyDf.as(\"source\"),\n",
        "    \"source.id = target.id\")\n",
        "  .whenMatched().updateAll()\n",
        "  .whenNotMatched().insertAll()\n",
        "  .execute()"
      ],
      "metadata": {
        "id": "it4OEIv6KNtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temporary View in Databricks"
      ],
      "metadata": {
        "id": "FEREII8uKSyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %scala\n",
        "val file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "\n",
        "val df = spark.read.format(\"csv\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"sep\", \",\")\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "tPqhFs0kKW5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global View in Databricks"
      ],
      "metadata": {
        "id": "pg4zMVCxKbyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %scala\n",
        "val file_location = \"/FileStore/tables/emp_data1-3.csv\"\n",
        "\n",
        "val df = spark.read.format(\"csv\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"sep\", \",\")\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "6DvfGY40KbL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df.createOrReplaceGlobalTempView(\"df_globalview\")\n"
      ],
      "metadata": {
        "id": "MA_TJDZrKjbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "select * from df_globalview"
      ],
      "metadata": {
        "id": "9PRzU3ssKlwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "select * from global_temp.df_globalview"
      ],
      "metadata": {
        "id": "t8VOinwSKo9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vacuum Delta Table\n"
      ],
      "metadata": {
        "id": "7TyfP5QsKqxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VACUUM\n",
        "<table_name> RETAIN 168 HOURS"
      ],
      "metadata": {
        "id": "5qzg20_0KuuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto Optimize Delta Table in Databricks"
      ],
      "metadata": {
        "id": "PT4LCod5K0I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "CREATE TABLE employee(empno bigint, ename  string, designation string, manager string,\n",
        "                      hire_date date, sal double, deptno bigint, location string)\n",
        "TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)"
      ],
      "metadata": {
        "id": "Eoq-aB4YK8Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable for Existing Created Table\n",
        "\n",
        "%sql\n",
        "\n",
        "ALTER TABLE employee SET TBLPROPERTIES\n",
        " (delta.autoOptimize.optimizeWrite = true,\n",
        "  delta.autoOptimize.autoCompact = true)\n"
      ],
      "metadata": {
        "id": "ZVqRydZRLI4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable Auto Optimize in Spark Session\n",
        "%sql\n",
        "set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;\n",
        "set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;"
      ],
      "metadata": {
        "id": "Trvacx-mLP0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "OPTIMIZE employee;"
      ],
      "metadata": {
        "id": "QRzOXjwZLkj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DELTA TABLE Utility Commands"
      ],
      "metadata": {
        "id": "-PTaKbHbLwej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE Managed Table\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE delta_table_name (\n",
        "……\n",
        ") USING delta\n",
        "\"\"\")\n",
        "\n",
        "# CREATE External Table\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE delta_table_name (\n",
        "……\n",
        ") USING delta\n",
        "LOCATION '<path>'\n",
        "\"\"\")\n",
        "\n",
        "# SELECT FROM Delta File\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM delta.`<path>`\n",
        "\"\"\")\n",
        "\n",
        "# DESCRIBE HISTORY\n",
        "spark.sql(\"\"\"\n",
        "DESCRIBE HISTORY delta.'<path>'\n",
        "\"\"\")\n",
        "\n",
        "# UPDATE\n",
        "spark.sql(\"\"\"\n",
        "UPDATE <table_name> SET <column_name> = <value> WHERE <CONDITION>\n",
        "\"\"\")\n",
        "\n",
        "# MERGE\n",
        "spark.sql(\"\"\"\n",
        "MERGE INTO <target_delta_table>\n",
        "USING <source_table>\n",
        "ON <merge_condition>\n",
        "WHEN MATCHED THEN UPDATE *\n",
        "\"\"\")\n",
        "\n",
        "# DELETE\n",
        "spark.sql(\"\"\"\n",
        "DELETE FROM delta.'<path>'\n",
        "\"\"\")\n",
        "\n",
        "# CONVERT Non-Partition Table\n",
        "spark.sql(\"\"\"\n",
        "CONVERT TO DELTA parquet.'<parquet_table_path>'\n",
        "\"\"\")\n",
        "\n",
        "# CONVERT Partition Table\n",
        "spark.sql(\"\"\"\n",
        "CONVERT TO DELTA parquet.'<parquet_table_path>' PARTITIONED BY (<column name> <column datatype>)\n",
        "\"\"\")\n",
        "\n",
        "# VACUUM\n",
        "spark.sql(\"\"\"\n",
        "VACUUM delta.'<path>'\n",
        "\"\"\")\n",
        "\n",
        "# OPTIMIZE\n",
        "spark.sql(\"\"\"\n",
        "OPTIMIZE <delta_table_name>\n",
        "\"\"\")\n",
        "\n",
        "# CLONE Deep Clone\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE IF NOT EXISTS delta.'<target_path>' CLONE delta.'<source_path>'\n",
        "\"\"\")\n",
        "\n",
        "# CLONE Shallow Clone\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE IF NOT EXISTS delta delta.'<target_path>' SHALLOW CLONE delta.'<source_path>'\n",
        "\"\"\")\n",
        "\n",
        "# RESTORE to Version\n",
        "spark.sql(\"\"\"\n",
        "RESTORE TABLE delta.'<path>' TO VERSION AS OF <version>\n",
        "\"\"\")\n",
        "\n",
        "# RESTORE to Timestamp\n",
        "spark.sql(\"\"\"\n",
        "RESTORE TABLE delta.'<path>' TO TIMESTAMP AS OF <timestamp>\n",
        "\"\"\")\n",
        "\n",
        "# DESCRIBE DETAIL\n",
        "spark.sql(\"\"\"\n",
        "DESCRIBE DETAIL delta.'<path>'\n",
        "\"\"\")\n",
        "\n",
        "# SHOW CREATE TABLE\n",
        "spark.sql(\"\"\"\n",
        "SHOW CREATE TABLE <delta_table_name>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "bn_pFJw0LywK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Bloom Filter Index"
      ],
      "metadata": {
        "id": "idVObZkbLC59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE testdb.bloomtabletest (\n",
        "empno bigint,\n",
        "ename string,\n",
        "designation string,\n",
        "manager string,\n",
        "hire_date date,\n",
        "sal double,\n",
        "depno bigint,\n",
        "location string\n",
        ") Using DELTA\n",
        "Location '/mnt/bdp/bloomtabletest';"
      ],
      "metadata": {
        "id": "qSd4uBxMMe6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE BLOOMFILTER INDEX\n",
        "ON TABLE bloomtabletest\n",
        "FOR COLUMNS(ename OPTIONS (fpp=0.1, numItems=1000000);"
      ],
      "metadata": {
        "id": "eJ0pK33LMiGo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}