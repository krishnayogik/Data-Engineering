{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This will guide you through setting up and using Databricks Asset Bundles to manage Delta Live Tables pipelines.\n",
        "\n",
        "## PART 1- Delta Live Tables - Simple Tutorial\n",
        "## PART 2 - Delta Live Tables with Asset Bundles"
      ],
      "metadata": {
        "id": "hdTWzWWmAwB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1 - Delta Live Tables with Databricks - Simplified Tutorial ✅\n",
        "\n",
        "This notebook will guide you through the process of creating a Delta Live Table (DLT) pipeline using Databricks Asset Bundles, complete with explanations for each step.\n",
        "\n",
        "### Step 1: Set Up Your Databricks Environment\n",
        "\n",
        "#### 1.1 Create a Cluster\n",
        "1. **Log in to Databricks:**\n",
        "   - Open your web browser and navigate to your Databricks workspace.\n",
        "   - Log in with your credentials.\n",
        "\n",
        "2. **Create a New Cluster:**\n",
        "   - On the left sidebar, click on \"Clusters.\"\n",
        "   - Click on the \"Create Cluster\" button.\n",
        "   - Enter a name for your cluster (e.g., `DeltaLiveCluster`).\n",
        "   - Choose the Databricks runtime version (e.g., 10.4 LTS).\n",
        "   - Select the appropriate instance type and number of workers based on your subscription.\n",
        "   - Click on \"Create Cluster.\"\n",
        "\n",
        "#### 1.2 Create a Notebook\n",
        "1. **Create a New Notebook:**\n",
        "   - In the workspace, create a new notebook where you'll run the code.\n"
      ],
      "metadata": {
        "id": "Bk1Ava3FAzrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Define the Input Data\n",
        "\n",
        "#### 2.1 Prepare Sample JSON Data\n",
        "1. **Create a Sample JSON File:**\n",
        "   - Open a text editor on your computer.\n",
        "   - Copy and paste the following JSON content into the file:\n",
        "```json\n",
        "[\n",
        "  {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n",
        "  {\"id\": 2, \"name\": \"Bob\", \"age\": 25},\n",
        "  {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n",
        "]"
      ],
      "metadata": {
        "id": "6i5OK2PFA4rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Upload JSON File to DBFS\n",
        "\n",
        "#### Upload the File:\n",
        "- In Databricks, click on \"Data\" on the left sidebar.\n",
        "- Click on \"Add Data\" > \"Upload File.\"\n",
        "- Click on \"Browse,\" select the input.json file from your computer, and click \"Open.\"\n",
        "- Choose a location to upload the file (e.g., /FileStore/tables/input.json).\n",
        "- Click on \"Next\" and then \"Preview & Confirm.\""
      ],
      "metadata": {
        "id": "uzMRVSW4A9LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a Delta Live Table Pipeline\n",
        "#### 3.1 Create and Configure a Delta Live Table Pipeline\n",
        "- Navigate to Delta Live Tables:\n",
        "- On the left sidebar, click on \"Delta Live Tables.\"\n",
        "- Click on \"Create Pipeline.\"\n",
        "- Enter a name for your pipeline (e.g., SimpleDeltaLiveTablePipeline).\n",
        "- Choose the cluster you created earlier (DeltaLiveCluster).\n",
        "- Set the Target schema to default (or any database name you prefer).\n",
        "- In the \"Notebook Libraries\" section, add the path to the notebook you created earlier (/Workspace/DeltaLiveTableTutorial)."
      ],
      "metadata": {
        "id": "pO0WFbXSBLJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define the Pipeline Code\n",
        "### 4.1 Write the Code in Your Notebook\n",
        "- Open the Notebook:\n",
        "- Navigate to the notebook you created (DeltaLiveTableTutorial).\n",
        "- Copy and paste the following code into the notebook cells:"
      ],
      "metadata": {
        "id": "5gpzf8RKBgNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EYgjeCm9nrQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "import dlt\n",
        "\n",
        "# Step 1: Read the input JSON file\n",
        "input_path = \"/dbfs/FileStore/tables/input.json\"\n",
        "\n",
        "# Define the input data as a Delta Live Table\n",
        "@dlt.table\n",
        "def input_data():\n",
        "    return (spark.read\n",
        "                .json(input_path)\n",
        "                .select(col(\"id\"), col(\"name\"), col(\"age\")))\n",
        "\n",
        "# Step 2: Perform a simple transformation\n",
        "@dlt.table\n",
        "def transformed_data():\n",
        "    return (dlt.read(\"input_data\")\n",
        "                .withColumn(\"age_in_5_years\", col(\"age\") + 5))\n",
        "\n",
        "# Step 3: Write the transformed data to a Delta table\n",
        "@dlt.table\n",
        "def output_data():\n",
        "    return dlt.read(\"transformed_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explanation:\n",
        "- We read the JSON file from DBFS and define it as a Delta Live Table named input_data.\n",
        "- We perform a simple transformation by adding 5 years to the age column and store the result in a Delta Live Table named transformed_data.\n",
        "- We write the transformed data to a Delta table named output_data."
      ],
      "metadata": {
        "id": "N8gMmsemBnSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Deploy and Run the Pipeline\n",
        "#### 5.1 Deploy the Pipeline\n",
        "##### Deploy:\n",
        "\n",
        "- Go back to the Delta Live Tables UI.\n",
        "- Click on the pipeline you created (SimpleDeltaLiveTablePipeline).\n",
        "- Click \"Start\" to deploy and run the pipeline.\n",
        "\n",
        "#### 5.2 Run the Pipeline\n",
        "##### Monitor the Pipeline:\n",
        "- Monitor the progress of your pipeline in the Delta Live Tables UI.\n",
        "- You can see the status of each table and the overall pipeline."
      ],
      "metadata": {
        "id": "XAKQfbbUBwMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Verify the Output\n",
        "#### 6.1 Query the Delta Table\n",
        "##### Run a Query:\n",
        "- Open a new notebook or use the existing one.\n",
        "- Run the following SQL query to verify the output:"
      ],
      "metadata": {
        "id": "zigL8PKDCEY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT * FROM transformed_data;"
      ],
      "metadata": {
        "id": "wri5c4NnBtp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Schedule the Pipeline (Optional)\n",
        "#### 7.1 Set Up Scheduling\n",
        "##### Configure Scheduling:\n",
        "- In the Delta Live Tables UI, click on your pipeline (SimpleDeltaLiveTablePipeline).\n",
        "- Click on \"Edit Settings.\"\n",
        "- Under the \"Schedule\" section, configure the frequency and timing for the pipeline to run automatically.\n",
        "- Click \"Save.\""
      ],
      "metadata": {
        "id": "CSWSSrVHCPKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have successfully created, deployed, and run a Delta Live Table pipeline in Databricks. This tutorial covers the basics, and you can now explore more advanced features and configurations based on your requirements."
      ],
      "metadata": {
        "id": "dvCxXmvTCZNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2 - Delta Live Tables with Asset Bundles ✅"
      ],
      "metadata": {
        "id": "MbMPKyFNDf2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial will guide you through the process of creating a Delta Live Table (DLT) pipeline using Databricks Asset Bundles, complete with explanations for each step.\n"
      ],
      "metadata": {
        "id": "WVpK65c4IBX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements\n",
        "- Databricks CLI version 0.218.0 or above.\n",
        "- The remote workspace must have workspace files enabled.\n",
        "- (Optional) Install a Python module to support local pipeline development.\n"
      ],
      "metadata": {
        "id": "YQJXBZSDLQDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Set up authentication\n",
        "\n",
        "Use the Databricks CLI to initiate OAuth token management locally by running the following command for each target workspace:\n",
        "\n",
        "```bash\n",
        "databricks auth login --host <workspace-url>\n"
      ],
      "metadata": {
        "id": "UmNLJmBrLQNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the on-screen instructions to log in to your Databricks workspace.\n",
        "\n"
      ],
      "metadata": {
        "id": "pjun9-2KLQVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 2: Create the bundle\n",
        "\n",
        "Switch to a directory on your local development machine and use the Databricks CLI to run the bundle init command:\n",
        "\n",
        "\n",
        "```\n",
        "databricks bundle init\n"
      ],
      "metadata": {
        "id": "8oWoC3v0LYdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 3: Explore the bundle\n",
        "\n",
        "Switch to the root directory of your newly created bundle and open this directory with your preferred IDE. Files of particular interest include:\n",
        "- `databricks.yml`\n",
        "- `resources/<project-name>_job.yml`\n",
        "- `resources/<project-name>_pipeline.yml`\n",
        "- `src/dlt_pipeline.ipynb`\n"
      ],
      "metadata": {
        "id": "l0VecXfmLiUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Validate the project’s bundle configuration file\n",
        "\n",
        "From the root directory, use the Databricks CLI to run the bundle validate command:\n",
        "\n",
        "```bash\n",
        "databricks bundle validate\n"
      ],
      "metadata": {
        "id": "W_b5hUgTLl3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Deploy the local project to the remote workspace\n",
        "\n",
        "Use the Databricks CLI to run the bundle deploy command:\n",
        "\n",
        "``` databricks bundle deploy -t dev ```\n"
      ],
      "metadata": {
        "id": "YAii350FLoM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the local notebook and pipeline were deployed in your Databricks workspace.\n",
        "\n"
      ],
      "metadata": {
        "id": "1DMTywP3L0qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Run the deployed project\n",
        "\n",
        "From the root directory, use the Databricks CLI to run the bundle run command:\n",
        "\n",
        "```\n",
        "databricks bundle run -t dev <project-name>_pipeline\n",
        "```"
      ],
      "metadata": {
        "id": "2lCMyftjL4gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open your Databricks workspace using the Update URL provided in the terminal."
      ],
      "metadata": {
        "id": "U9exhummL_yD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Clean up\n",
        "\n",
        "From the root directory, use the Databricks CLI to run the bundle destroy command:\n",
        "\n",
        "```\n",
        "databricks bundle destroy -t dev\n",
        "```"
      ],
      "metadata": {
        "id": "2nRWsrGGL7SA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the deletion requests when prompted."
      ],
      "metadata": {
        "id": "gXPKAJ-yMDBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Create the bundle manually\n",
        "\n",
        "Create or identify an empty directory on your development machine. Create a file named `dlt-wikipedia-python.py` in this directory with the following code:"
      ],
      "metadata": {
        "id": "2QO3yTDtMF-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "json_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n",
        "\n",
        "@dlt.table(comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\")\n",
        "def clickstream_raw():\n",
        "    return spark.read.format(\"json\").load(json_path)\n",
        "\n",
        "@dlt.table(comment=\"Wikipedia clickstream data cleaned and prepared for analysis.\")\n",
        "@dlt.expect(\"valid_current_page_title\", \"current_page_title IS NOT NULL\")\n",
        "@dlt.expect_or_fail(\"valid_count\", \"click_count > 0\")\n",
        "def clickstream_prepared():\n",
        "    return (\n",
        "        dlt.read(\"clickstream_raw\")\n",
        "            .withColumn(\"click_count\", expr(\"CAST(n AS INT)\"))\n",
        "            .withColumnRenamed(\"curr_title\", \"current_page_title\")\n",
        "            .withColumnRenamed(\"prev_title\", \"previous_page_title\")\n",
        "            .select(\"current_page_title\", \"click_count\", \"previous_page_title\")\n",
        "    )\n",
        "\n",
        "@dlt.table(comment=\"A table containing the top pages linking to the Apache Spark page.\")\n",
        "def top_spark_referrers():\n",
        "    return (\n",
        "        dlt.read(\"clickstream_prepared\")\n",
        "            .filter(expr(\"current_page_title == 'Apache_Spark'\"))\n",
        "            .withColumnRenamed(\"previous_page_title\", \"referrer\")\n",
        "            .sort(desc(\"click_count\"))\n",
        "            .select(\"referrer\", \"click_count\")\n",
        "            .limit(10)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "v1VxZX0SLNk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Add a bundle configuration schema file to the project\n",
        "\n",
        "Generate the Databricks Asset Bundle configuration JSON schema file:\n",
        "\n",
        "```\n",
        "databricks bundle schema > bundle_config_schema.json\n",
        "```"
      ],
      "metadata": {
        "id": "p02qb0CAMLGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the following comment to the beginning of your bundle configuration file:\n",
        "\n",
        "```yaml-language-server: $schema=bundle_config_schema.json```"
      ],
      "metadata": {
        "id": "eFXVZL8LMQAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Set up authentication\n",
        "\n",
        "Set up authentication between the Databricks CLI on your development machine and your Databricks workspace using the same command as in Step 1."
      ],
      "metadata": {
        "id": "MlKManr2MXPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Add a bundle configuration file to the project\n",
        "\n",
        "Create a `databricks.yml` file with the following content, replacing `<workspace-url>` with your workspace URL:\n",
        "\n",
        "```\n",
        "# yaml-language-server: $schema=bundle_config_schema.json\n",
        "bundle:\n",
        "  name: dlt-wikipedia\n",
        "\n",
        "resources:\n",
        "  pipelines:\n",
        "    dlt-wikipedia-pipeline:\n",
        "      name: dlt-wikipedia-pipeline\n",
        "      development: true\n",
        "      continuous: false\n",
        "      channel: \"CURRENT\"\n",
        "      photon: false\n",
        "      libraries:\n",
        "        - notebook:\n",
        "            path: ./dlt-wikipedia-python.py\n",
        "      edition: \"ADVANCED\"\n",
        "      clusters:\n",
        "        - label: \"default\"\n",
        "          num_workers: 1\n",
        "\n",
        "targets:\n",
        "  development:\n",
        "    workspace:\n",
        "      host: <workspace-url>\n",
        "```"
      ],
      "metadata": {
        "id": "yCQ1WMbzMboI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Validate the project's bundle configuration file\n",
        "\n",
        "Use the Databricks CLI to run the bundle validate command:\n",
        "\n",
        "```databricks bundle validate```"
      ],
      "metadata": {
        "id": "rmB-p2EuMgvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Deploy the local project to the remote workspace\n",
        "\n",
        "Use the Databricks CLI to run the bundle deploy command:\n",
        "\n",
        "```databricks bundle deploy -t development```"
      ],
      "metadata": {
        "id": "reGEEeiUM3Z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the local notebook and pipeline were deployed in your Databricks workspace."
      ],
      "metadata": {
        "id": "xi-BpH9-M9G9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Run the deployed project\n",
        "\n",
        "Use the Databricks CLI to run the bundle run command:\n",
        "\n",
        "```databricks bundle run -t development dlt-wikipedia-pipeline```"
      ],
      "metadata": {
        "id": "7MHHHwTGNAr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Clean up\n",
        "\n",
        "Use the Databricks CLI to run the bundle destroy command:\n",
        "\n",
        "```databricks bundle destroy -t development```"
      ],
      "metadata": {
        "id": "OsrjG9r8NN9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the deletion requests when prompted."
      ],
      "metadata": {
        "id": "YwexPBZFNTYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This completes the tutorial"
      ],
      "metadata": {
        "id": "mE1kHCHJNWJ-"
      }
    }
  ]
}